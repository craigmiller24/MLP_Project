{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanFlightData(airport):\n",
    "    # Reads each of the raw data files, drops any rows with missing values, duplicates, or rows that are not from the target airport, and saves the cleaned data to a new file in the clean directory\n",
    "    for yy in range(10,19):\n",
    "        for mm in range(1,13):\n",
    "            if yy == 18 and mm == 8:\n",
    "                break\n",
    "            if mm < 10:\n",
    "                mm = f'0{mm}'\n",
    "\n",
    "            df = pd.read_csv(f'data/raw/{airport}/{airport}_{mm}_{yy}.csv', sep=',')\n",
    "\n",
    "            df = df.drop_duplicates()\n",
    "\n",
    "            # Only require flights from the target airport\n",
    "            df = df.drop(df[df['ORIGIN'] != f'{airport}'].index)\n",
    "\n",
    "            # Where delayed, but no delay reason is given, drop the row\n",
    "            df = df[~((df['DEP_DELAY_GROUP'] > 0) & df[['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']].isna().any(axis=1))]\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            df.to_csv(f'data/clean/{airport}/{airport}_{mm}_{yy}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeFiles(airport):\n",
    "    # Concatenates all of the cleaned data files into one large dataset\n",
    "    full_df = pd.DataFrame()\n",
    "\n",
    "    # Read in the cleaned datafiles spanning from January 2010 to July 2018 (Files names follow convention e.g. 'JFK_MM_YY.csv')\n",
    "    for yy in range(10,19):\n",
    "        for mm in range(1,13):\n",
    "            if yy == 18 and mm == 8:\n",
    "                break\n",
    "            if mm < 10:\n",
    "                mm = f'0{mm}'\n",
    "\n",
    "            df = pd.read_csv(f'data/clean/{airport}/{airport}_{mm}_{yy}.csv', sep=',')\n",
    "            full_df = pd.concat([full_df, df])\n",
    "            full_df.to_csv(f'data/clean/{airport}/{airport}_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanWeatherData(airport):\n",
    "    # Reads the weather data file, drops any rows with missing values, and saves the cleaned data to a new file in the clean directory\n",
    "    df = pd.read_csv(f'data/Weather/{airport}_Weather_cleaned.csv', sep=',')\n",
    "\n",
    "    # Drop rows with missing/duplicate values and only keep the columns we need\n",
    "    df = df.fillna(0)    \n",
    "    df = df.drop_duplicates()\n",
    "    df = df[['DATE','HOURLYVISIBILITY','HOURLYDRYBULBTEMPF','HOURLYWETBULBTEMPF','HOURLYDewPointTempF','HOURLYRelativeHumidity','HOURLYWindSpeed','HOURLYStationPressure','HOURLYSeaLevelPressure','HOURLYPrecip','HOURLYAltimeterSetting','HOURLYWindDirectionSin','HOURLYWindDirectionCos','HOURLYPressureTendencyIncr','HOURLYPressureTendencyDecr','HOURLYPressureTendencyCons']]\n",
    "    \n",
    "    # Override file with the cleaned weather data\n",
    "    df.to_csv(f'data/Weather/{airport}_Weather_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FormatTimes(full_df):\n",
    "    # Reformatting departure times to date times\n",
    "    full_df['CRS_DEP_TIME'] = full_df['CRS_DEP_TIME'].apply(lambda x: f\"{int(x):04d}\")\n",
    "    full_df['DEP_TIME'] = full_df['DEP_TIME'].apply(lambda x: f\"{int(x):04d}\")\n",
    "    full_df['CRS_DEP_TIME'] = full_df['CRS_DEP_TIME'].apply(lambda x: f\"{x[:2]}:{x[2:]}:00\")\n",
    "    full_df['DEP_TIME'] = full_df['DEP_TIME'].apply(lambda x: f\"{x[:2]}:{x[2:]}:00\")\n",
    "\n",
    "    # Reformatting FL_DATE to correct date-time format by removing AM/PM\n",
    "    full_df['FL_DATE'] = full_df['FL_DATE'].str.replace(r'AM|PM', '', regex=True)\n",
    "\n",
    "    # Convert datatype to date time\n",
    "    full_df['FL_DATE'] = pd.to_datetime(full_df['FL_DATE'], format='mixed')\n",
    "    \n",
    "    # Handle \"24:00:00\" in DEP_TIME\n",
    "    mask1 = full_df['CRS_DEP_TIME'] == '24:00:00'  # Find rows with \"24:00:00\"\n",
    "    mask2 = full_df['DEP_TIME'] == '24:00:00'  # Find rows with \"24:00:00\"\n",
    "\n",
    "    # Replace \"24:00:00\" with \"00:00:00\"\n",
    "    full_df.loc[mask1, 'DEP_TIME'] = '00:00:00'\n",
    "    full_df.loc[mask2, 'CRS_DEP_TIME'] = '00:00:00'\n",
    "\n",
    "    # Add one day to FL_DATE where DEP_TIME was \"24:00:00\"\n",
    "    full_df.loc[mask1, 'FL_DATE'] += pd.Timedelta(days=1)\n",
    "\n",
    "    # Convert to proper datetime format\n",
    "    full_df['DEP_DATE_TIME'] = pd.to_datetime(full_df['FL_DATE'].dt.date.astype(str) + ' ' + full_df['CRS_DEP_TIME'])\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeDatasets(airport):\n",
    "    full_df = pd.read_csv(f'data/clean/{airport}/{airport}_full.csv', sep=',')\n",
    "    weather_df = pd.read_csv(f'data/Weather/{airport}_Weather_cleaned.csv', sep=',')\n",
    "\n",
    "    full_df = FormatTimes(full_df)\n",
    "\n",
    "    # Ensure datetime format\n",
    "    weather_df['DATE'] = pd.to_datetime(weather_df['DATE'])\n",
    "    full_df['DEP_DATE_TIME'] = pd.to_datetime(full_df['DEP_DATE_TIME'])\n",
    "\n",
    "    # Round DEP_DATE_TIME to the nearest hour for merging\n",
    "    full_df['DEP_DATE_TIME_HOURLY'] = full_df['DEP_DATE_TIME'].dt.round('H')\n",
    "\n",
    "    # Sort both dataframes by time (required for merge_asof)\n",
    "    weather_df = weather_df.sort_values('DATE')\n",
    "    full_df = full_df.sort_values('DEP_DATE_TIME_HOURLY')\n",
    "\n",
    "    # Merge, taking the nearest past weather observation\n",
    "    merged_df = pd.merge_asof(\n",
    "        full_df, \n",
    "        weather_df, \n",
    "        left_on='DEP_DATE_TIME_HOURLY',  # Use rounded departure time for merging\n",
    "        right_on='DATE', \n",
    "        direction='backward'  # Match the most recent weather before departure\n",
    "    )\n",
    "\n",
    "    # Drop the FL_DATE column as no longer needed and replace it with more informative DEP_DATE_TIME_HOURLY\n",
    "    merged_df.drop(columns=['FL_DATE'], inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    col_data = merged_df.pop(\"DEP_DATE_TIME_HOURLY\")\n",
    "    merged_df.insert(0, \"DEP_DATE_TIME_HOURLY\", col_data)  # Insert it at index 0 (first column)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TidyData(merged_df):\n",
    "    # Remove cancelled flights with delay group != 0\n",
    "    merged_df = merged_df[(merged_df['CANCELLED'] == 0)]\n",
    "\n",
    "    # Convert 'HOURLYDRYBULBTEMPF', 'HOURLYWETBULBTEMPF' and 'HOURLYDewPointTempF' values from farhenheit to celsius\n",
    "    merged_df['HOURLYDRYBULBTEMPC'] = (merged_df['HOURLYDRYBULBTEMPF'] - 32) * 5/9\n",
    "    merged_df['HOURLYWETBULBTEMPC'] = (merged_df['HOURLYWETBULBTEMPF'] - 32) * 5/9\n",
    "    merged_df['HOURLYDewPointTempC'] = (merged_df['HOURLYDewPointTempF'] - 32) * 5/9\n",
    "\n",
    "    # Drop the original columns\n",
    "    merged_df.drop(columns=['HOURLYDRYBULBTEMPF', 'HOURLYWETBULBTEMPF', 'HOURLYDewPointTempF'], inplace=True)\n",
    "\n",
    "    # Show only 'DEP_DATE_TIME_HOURLY','CRS_DEP_TIME','DEP_TIME','DEP_DATE_TIME','DATE' columns\n",
    "    merged_df[['DEP_DATE_TIME_HOURLY','CRS_DEP_TIME','DEP_TIME','DEP_DATE_TIME','DATE']]\n",
    "    merged_df.drop(columns=['DEP_DATE_TIME_HOURLY','CRS_DEP_TIME','DATE'], inplace=True)\n",
    "\n",
    "    # Move DEP_DATE_TIME and DEP_TIME to the front\n",
    "    col_data = merged_df.pop(\"DEP_DATE_TIME\")\n",
    "    merged_df.insert(0, \"DEP_DATE_TIME\", col_data)\n",
    "    col_data = merged_df.pop(\"DEP_TIME\")\n",
    "    merged_df.insert(1, \"DEP_TIME\", col_data)\n",
    "\n",
    "    # Rename DEP_TIME to ACC_DEP_TIME\n",
    "    merged_df.rename(columns={'DEP_TIME': 'ACC_DEP_TIME'}, inplace=True)\n",
    "\n",
    "    # Removing 'early' classification\n",
    "    merged_df.loc[merged_df['DEP_DELAY_GROUP'] <0, 'DEP_DELAY_GROUP'] = 0\n",
    "\n",
    "    # Extract time-related features from `DEP_DATE_TIME`\n",
    "    merged_df['DEP_HOUR'] = merged_df['DEP_DATE_TIME'].dt.hour\n",
    "    merged_df['DEP_DAY'] = merged_df['DEP_DATE_TIME'].dt.day\n",
    "    merged_df['DEP_MONTH'] = merged_df['DEP_DATE_TIME'].dt.month\n",
    "    merged_df['DEP_WEEKDAY'] = merged_df['DEP_DATE_TIME'].dt.weekday\n",
    "\n",
    "    # Convert DEP_DELAY_NEW, DEP_DELAY_GROUP, CANCELLED, CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY and LATE_AIRCRAFT_DELAY to int\n",
    "    merged_df['DEP_DELAY_NEW'] = merged_df['DEP_DELAY_NEW'].astype(int)\n",
    "    merged_df['DEP_DELAY_GROUP'] = merged_df['DEP_DELAY_GROUP'].astype(int)\n",
    "    merged_df['CANCELLED'] = merged_df['CANCELLED'].astype(int)\n",
    "    merged_df['CARRIER_DELAY'] = merged_df['CARRIER_DELAY'].astype(int)\n",
    "    merged_df['WEATHER_DELAY'] = merged_df['WEATHER_DELAY'].astype(int)\n",
    "    merged_df['NAS_DELAY'] = merged_df['NAS_DELAY'].astype(int)\n",
    "    merged_df['SECURITY_DELAY'] = merged_df['SECURITY_DELAY'].astype(int)\n",
    "    merged_df['LATE_AIRCRAFT_DELAY'] = merged_df['LATE_AIRCRAFT_DELAY'].astype(int)\n",
    "\n",
    "    # Reindex the DataFrame to ensure that the columns are in the correct order\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessData(airport):\n",
    "    # Read and clean the raw individual flight data files for each airport\n",
    "    CleanFlightData(airport)\n",
    "    # Merge the cleaned data files into one large dataset\n",
    "    MergeFiles(airport)\n",
    "    # Clean Weather Data\n",
    "    CleanWeatherData(airport)\n",
    "    # Merge the full dataset with the weather data\n",
    "    merged_df = MergeDatasets(airport)\n",
    "\n",
    "    merged_df = TidyData(merged_df)\n",
    "    merged_df.to_csv(f'data/complete/{airport}_final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airports = ['JFK','LAX','EWR','MIA','ORD']\n",
    "#for airport in airports:\n",
    "#    ProcessData(airport)\n",
    "ProcessData('ORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
